{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import multiprocessing as mp\n",
    "from dask import distributed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "colNames = ['lon', 'lat', 'speed', 'uuid', 'roadtype', 'level', 'delay', 'length', 'epoch', 'datetime', \n",
    "        'CO2-Atm_base', 'CO2-Eq_base','CO_base','NOx_base','VOC_base','SO2_base','NH3_base','PM10_base','PM2.5_base',\n",
    "        'CO2-Atm_ldv', 'CO2-Eq_ldv','CO_ldv','NOx_ldv','VOC_ldv','SO2_ldv','NH3_ldv','PM10_ldv','PM2.5_ldv',\n",
    "        'CO2-Atm_bus', 'CO2-Eq_bus','CO_bus','NOx_bus','VOC_bus','SO2_bus','NH3_bus','PM10_bus','PM2.5_bus',\n",
    "        'CO2-Atm_taxi', 'CO2-Eq_taxi','CO_taxi','NOx_taxi','VOC_taxi','SO2_taxi','NH3_taxi','PM10_taxi','PM2.5_taxi']\n",
    "cluster = distributed.LocalCluster(n_workers=mp.cpu_count(), threads_per_worker = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run this after doing the speed curve creation -> pollutant calculation. The goal of this code is simply to create buckets for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/gaoag/Documents/Junior/urap-erg/all notebooks, start to end/analysis_all_policies.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d8e6208fbb37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./analysis_all_policies.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolNames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/dataframe/io/csv.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m                            \u001b[0menforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menforce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massume_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massume_missing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                            \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m                            **kwargs)\n\u001b[0m\u001b[1;32m    430\u001b[0m     read.__doc__ = READ_DOC_TEMPLATE.format(reader=reader_name,\n\u001b[1;32m    431\u001b[0m                                             file_type=file_type)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/dataframe/io/csv.py\u001b[0m in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, collection, lineterminator, compression, sample, enforce, assume_missing, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                                   \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                                   \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                                   **(storage_options or {}))\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/bytes/core.py\u001b[0m in \u001b[0;36mread_bytes\u001b[0;34m(urlpath, delimiter, not_zero, blocksize, sample, compression, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 raise ValueError('Cannot read compressed files (%s) in byte chunks,'\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/bytes/core.py\u001b[0m in \u001b[0;36mlogical_size\u001b[0;34m(self, path, compression)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/dask/bytes/local.py\u001b[0m in \u001b[0;36msize\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;34m\"\"\"Size in bytes of the file at path\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trim_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_pyarrow_filesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/gaoag/Documents/Junior/urap-erg/all notebooks, start to end/analysis_all_policies.csv'"
     ]
    }
   ],
   "source": [
    "df = dd.read_csv('./analysis_all_policies.csv', header=None, names=colNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the dask dataframe, do vol scaling. next, do bucketing, groupby and agg to sum by bucket. finally, do percentage diffs on the buckets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOLUME SCALING\n",
    "\n",
    "In the original pollutant analysis, we scaled the emissions of 1 car to the emissions of X cars (where X is the # of vehicles which fit on the line segment). However, of those X vehicles, we have to assume that some of them are no longer emitting, since we passed these electrification policies. The amount of vehicles effectively 'taken off the road' are calculated in https://paper.dropbox.com/doc/WAZE-API-5yK5F5OGXKAna1tGlJbYD under 'Vehicle Distribution Data'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in colNames:\n",
    "    if '_' not in col:\n",
    "        continue\n",
    "    elif col.split('_')[1] == 'ldv':\n",
    "        df[col] = df[col]*0.4803\n",
    "    elif col.split('_')[1] == 'bus':\n",
    "        df[col] = df[col]*0.9919\n",
    "    elif col.split('_')[1] == 'taxi':\n",
    "        df[col] = df[col]*0.9636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bucketing, 500x500. origin is the top left of the map\n",
    "#these boundaries are the max/min lat/lon in the entire dataset.\n",
    "#max_lat = 20.108879999999999 \n",
    "#origin_lat = 18.807134\n",
    "#max_lon = -98.195071\n",
    "#origin_lon = -99.662641\n",
    "\n",
    "#these boundaries are the max/min lat/lon for mexico city only.\n",
    "max_lat = 19.595892\n",
    "origin_lat = 19.220471\n",
    "max_lon = -98.828109\n",
    "origin_lon = -99.438230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaoag/anaconda3/lib/python3.6/site-packages/dask/dataframe/core.py:2067: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#first, we cut out all the entries in our dataframe which don't fit the desired latitude/longitude.\n",
    "df = df[(df['lat'] < max_lat) & (df['lat'] > origin_lat) & (df['lon'] < max_lon) & (df['lon'] > origin_lon)]\n",
    "\n",
    "#next, we assign each row an x_bucket or y_bucket based upon its latitude and longitude.\n",
    "lon_step_size = abs(max_lon - origin_lon)/500\n",
    "lat_step_size = abs(max_lat - origin_lat)/500\n",
    "df['x_bucket'] = df['lat'].apply(lambda x: abs(x - origin_lat)//lat_step_size)\n",
    "df['y_bucket'] = df['lon'].apply(lambda y: abs(y - origin_lon)//lon_step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we group by and sum across buckets. What this does: there will be many rows which fit within each bucket. For example, the bucket (0, 0) may have 6 data points that fit into that bucket. This sums those datapoints together to come up with a cumulative amount of emissions within that bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_to_agg = ['CO2-Atm_base', 'CO2-Eq_base','CO_base','NOx_base','VOC_base','SO2_base','NH3_base','PM10_base','PM2.5_base',\n",
    "        'CO2-Atm_ldv', 'CO2-Eq_ldv','CO_ldv','NOx_ldv','VOC_ldv','SO2_ldv','NH3_ldv','PM10_ldv','PM2.5_ldv',\n",
    "        'CO2-Atm_bus', 'CO2-Eq_bus','CO_bus','NOx_bus','VOC_bus','SO2_bus','NH3_bus','PM10_bus','PM2.5_bus',\n",
    "        'CO2-Atm_taxi', 'CO2-Eq_taxi','CO_taxi','NOx_taxi','VOC_taxi','SO2_taxi','NH3_taxi','PM10_taxi','PM2.5_taxi']\n",
    "\n",
    "grouped = df.groupby(['x_bucket', 'y_bucket'])[cols_to_agg].agg('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our dataframe has one row for each of the 500x500 = 250,000 buckets. In each 'bucket', designated by a unique x and y coordinate, there is a value for the total amount of emissions within that bucket, across all the different pollutant types and policy scenarios. What we care about is percentage differences between the policies and the baseline scenario. Thus, we add some columns that reflect percentage differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pollutant in ['CO2-Atm', 'CO2-Eq', 'CO', 'NOx', 'VOC', 'SO2', 'NH3', 'PM10', 'PM2.5']:\n",
    "    grouped['ldv_diff_' + pollutant] = (grouped[pollutant+'_ldv'] - grouped[pollutant+'_base'])/grouped[pollutant+'_base']\n",
    "    grouped['bus_diff_' + pollutant] = (grouped[pollutant+'_bus'] - grouped[pollutant+'_base'])/grouped[pollutant+'_base']\n",
    "    grouped['taxi_diff_' + pollutant] = (grouped[pollutant+'_taxi'] - grouped[pollutant+'_base'])/grouped[pollutant+'_base']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just write out to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./emissions_diffs/bus/bus-0.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdf = grouped[['bus_diff_CO2-Atm', 'bus_diff_CO2-Eq', 'bus_diff_CO', 'bus_diff_NOx', 'bus_diff_VOC', 'bus_diff_SO2', 'bus_diff_NH3', 'bus_diff_PM10', 'bus_diff_PM2.5','CO2-Atm_bus', 'CO2-Eq_bus','CO_bus','NOx_bus','VOC_bus','SO2_bus','NH3_bus','PM10_bus','PM2.5_bus']]\n",
    "tempdf.to_csv('./emissions_diffs/bus/bus_mxcity1-*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./emissions_diffs/ldv/ldv-0.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdf = grouped[['ldv_diff_CO2-Atm', 'ldv_diff_CO2-Eq', 'ldv_diff_CO', 'ldv_diff_NOx', 'ldv_diff_VOC', 'ldv_diff_SO2', 'ldv_diff_NH3', 'ldv_diff_PM10', 'ldv_diff_PM2.5']]\n",
    "tempdf.to_csv('./emissions_diffs/ldv/ldv_mxcity1-*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./emissions_diffs/taxi/taxi-0.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdf = grouped[['taxi_diff_CO2-Atm', 'taxi_diff_CO2-Eq', 'taxi_diff_CO', 'taxi_diff_NOx', 'taxi_diff_VOC', 'taxi_diff_SO2', 'taxi_diff_NH3', 'taxi_diff_PM10', 'taxi_diff_PM2.5']]\n",
    "tempdf.to_csv('./emissions_diffs/taxi/taxi_mxcity1-*.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
