{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bucketing, 500x500. origin is the top left of the map\n",
    "#these boundaries are the max/min lat/lon in the entire dataset.\n",
    "#max_lat = 20.108879999999999 \n",
    "#origin_lat = 18.807134\n",
    "#max_lon = -98.195071\n",
    "#origin_lon = -99.662641\n",
    "\n",
    "#these boundaries are the max/min lat/lon for mexico city only.\n",
    "max_lat = 19.595892\n",
    "origin_lat = 19.220471\n",
    "max_lon = -98.828109\n",
    "origin_lon = -99.438230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaoag/anaconda3/lib/python3.6/site-packages/dask/dataframe/core.py:2067: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#first, we cut out all the entries in our dataframe which don't fit the desired latitude/longitude.\n",
    "df = df[(df['lat'] < max_lat) & (df['lat'] > origin_lat) & (df['lon'] < max_lon) & (df['lon'] > origin_lon)]\n",
    "\n",
    "#next, we assign each row an x_bucket or y_bucket based upon its latitude and longitude.\n",
    "lon_step_size = abs(max_lon - origin_lon)/500\n",
    "lat_step_size = abs(max_lat - origin_lat)/500\n",
    "df['x_bucket'] = df['lat'].apply(lambda x: abs(x - origin_lat)//lat_step_size)\n",
    "df['y_bucket'] = df['lon'].apply(lambda y: abs(y - origin_lon)//lon_step_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, we group by and sum across buckets. What this does: there will be many rows which fit within each bucket. For example, the bucket (0, 0) may have 6 data points that fit into that bucket. This sums those datapoints together to come up with a cumulative amount of emissions within that bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_to_agg = ['CO2-Atm_base', 'CO2-Eq_base','CO_base','NOx_base','VOC_base','SO2_base','NH3_base','PM10_base','PM2.5_base',\n",
    "        'CO2-Atm_ldv', 'CO2-Eq_ldv','CO_ldv','NOx_ldv','VOC_ldv','SO2_ldv','NH3_ldv','PM10_ldv','PM2.5_ldv',\n",
    "        'CO2-Atm_bus', 'CO2-Eq_bus','CO_bus','NOx_bus','VOC_bus','SO2_bus','NH3_bus','PM10_bus','PM2.5_bus',\n",
    "        'CO2-Atm_taxi', 'CO2-Eq_taxi','CO_taxi','NOx_taxi','VOC_taxi','SO2_taxi','NH3_taxi','PM10_taxi','PM2.5_taxi']\n",
    "\n",
    "grouped = df.groupby(['x_bucket', 'y_bucket'])[cols_to_agg].agg('sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our dataframe has one row for each of the 500x500 = 250,000 buckets. In each 'bucket', designated by a unique x and y coordinate, there is a value for the total amount of emissions within that bucket, across all the different pollutant types and policy scenarios. What we care about is percentage differences between the policies and the baseline scenario. Thus, we add some columns that reflect percentage differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pollutant in ['CO2-Atm', 'CO2-Eq', 'CO', 'NOx', 'VOC', 'SO2', 'NH3', 'PM10', 'PM2.5']:\n",
    "    grouped['ldv_diff_' + pollutant] = (grouped[pollutant+'_ldv'] - grouped[pollutant+'_base'])\n",
    "    grouped['bus_diff_' + pollutant] = (grouped[pollutant+'_bus'] - grouped[pollutant+'_base'])\n",
    "    grouped['taxi_diff_' + pollutant] = (grouped[pollutant+'_taxi'] - grouped[pollutant+'_base'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just write out to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../emissions_diffs/bus/bus_mxcity1-0.csv']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdf = grouped[['bus_diff_CO2-Atm', 'bus_diff_CO2-Eq', 'bus_diff_CO', 'bus_diff_NOx', 'bus_diff_VOC', 'bus_diff_SO2', 'bus_diff_NH3', 'bus_diff_PM10', 'bus_diff_PM2.5','CO2-Atm_bus', 'CO2-Eq_bus','CO_bus','NOx_bus','VOC_bus','SO2_bus','NH3_bus','PM10_bus','PM2.5_bus']]\n",
    "tempdf.to_csv('../emissions_diffs/bus/bus_mxcity1-*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../emissions_diffs/ldv/ldv_mxcity1-0.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdf = grouped[['ldv_diff_CO2-Atm', 'ldv_diff_CO2-Eq', 'ldv_diff_CO', 'ldv_diff_NOx', 'ldv_diff_VOC', 'ldv_diff_SO2', 'ldv_diff_NH3', 'ldv_diff_PM10', 'ldv_diff_PM2.5']]\n",
    "tempdf.to_csv('../emissions_diffs/ldv/ldv_mxcity1-*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../emissions_diffs/taxi/taxi_mxcity1-0.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdf = grouped[['taxi_diff_CO2-Atm', 'taxi_diff_CO2-Eq', 'taxi_diff_CO', 'taxi_diff_NOx', 'taxi_diff_VOC', 'taxi_diff_SO2', 'taxi_diff_NH3', 'taxi_diff_PM10', 'taxi_diff_PM2.5']]\n",
    "tempdf.to_csv('../emissions_diffs/taxi/taxi_mxcity1-*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../emissions_diffs/base/base_mxcity1-0.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdf = grouped[['CO2-Atm_base', 'CO2-Eq_base','CO_base','NOx_base','VOC_base','SO2_base','NH3_base','PM10_base','PM2.5_base']]\n",
    "tempdf.to_csv('../emissions_diffs/base/base_mxcity1-*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
